# 10 - AI Integration Architecture

**Last Updated**: 2024-07-23

## 1. Core Principles
- **Agent Specialization**: We use a multi-agent system where each agent has a specific role. Gemini 2.5 Pro is the "Developer Agent" for generation, and Claude 3 Opus is the "Auditor Agent" for review. This separation of concerns ensures higher quality and security.
- **Orchestration**: A central `SuperClaude/ClaudeCode` service acts as an orchestrator. It manages the workflow between agents, ensuring that, for example, code generated by Gemini is automatically reviewed by Claude before being presented to the user.
- **Transparency**: The user interface will make it clear which agent is performing an action, providing transparency into the collaborative AI process.
- **Abstraction**: The frontend is completely decoupled from the specific AI model. It communicates with a dedicated integration layer (`SuperClaude/ClaudeCode`), not directly with Gemini. This allows for easier model versioning, testing, and future replacement.
- **Stateful Conversations**: All conversations are stateful and tied to a specific project. The integration layer is responsible for retrieving context, managing history, and constructing prompts accordingly.
- **Asynchronicity**: Long-running AI tasks (like generating a large file) must be handled asynchronously to prevent blocking the UI.

## 2. System Components
- **Next.js Frontend**: The user-facing application.
- **SuperClaude/ClaudeCode (AI Orchestrator)**: The central backend service. It routes requests to the appropriate agent, manages multi-agent workflows, and aggregates results.
- **Gemini 2.5 Pro (Developer Agent)**: The primary generative model for code, analysis, and conversational responses.
- **Claude 3 Opus (Auditor Agent)**: The primary review model for code quality, security analysis, and architectural compliance.
- **PostgreSQL Database**: Stores conversation history, project data, and review results.
- **Message Queue (Redis/RabbitMQ)**: Used for managing asynchronous tasks between the orchestrator and the agents, especially for long-running generation and review jobs.

## 3. Multi-Agent Workflow Patterns
The orchestrator will employ several workflow patterns depending on the user's request:

- **Pattern A: Sequential Review (Default)**
    1.  The user's request is sent to Gemini to generate the initial code or document.
    2.  The generated output is automatically passed to Claude for a quality and security review.
    3.  If Claude approves, the output is presented to the user.
    4.  If Claude finds issues, its feedback is given to Gemini, which attempts to refine the output. The refined output is then presented to the user with the review history.

- **Pattern B: Parallel Analysis (for Architectural Decisions)**
    1.  For a high-level request (e.g., "Design the database schema"), the orchestrator sends the request to both agents simultaneously.
    2.  Gemini generates a proposed implementation plan.
    3.  Claude generates a list of potential risks, benefits, and alternative approaches.
    4.  The orchestrator combines these two responses into a single, comprehensive answer for the user.

- **Pattern C: Checkpoint Reviews (for Critical Features)**
    1.  For a large feature, Gemini is instructed to develop it in stages.
    2.  At the end of each stage, the code is passed to Claude for a checkpoint review.
    3.  Development only proceeds to the next stage if Claude approves.

## 4. API Architecture
The API will be designed to support the orchestrator model.

- **Primary Endpoint**: `POST /api/v1/orchestrate`
    - This single endpoint handles most user interactions.
- **Request Body**:
    ```json
    {
      "workflow": "sequential_review", // or "parallel_analysis"
      "projectId": "123",
      "prompt": "The user's input text",
      "context": { ... }
    }
    ```
- **Response**: The API will use `Transfer-Encoding: chunked` to stream the response back. Each chunk will be a Server-Sent Event (SSE) with a specific type:
    - `event: text_chunk`: A piece of the AI's text response with agent identification (Developer/Auditor).
    - `event: tool_call`: A request for the user to confirm a tool action via the Command Palette.
    - `event: status_update`: A change in the AI's status (e.g., "generating", "reviewing", "complete").
    - `event: agent_switch`: Indicates transition between Developer Agent and Auditor Agent.
    - `event: stream_end`: Signals the end of the response.

## 5. Token Management & Cost Optimization
- **History Summarization**: For long conversations, the integration layer will not send the entire raw history to Gemini. It will use a smaller, cheaper model to summarize the earlier parts of the conversation, sending the summary plus the most recent turns.
- **Context Pruning**: The integration layer will intelligently prune the `context` object in the request, sending only the most relevant parts of a document to the AI, not the entire file.
- **Request Caching**: Identical requests made in a short period can be served from a cache instead of hitting the Gemini API.

## 6. Error Handling
- **AI Failures**: If the Gemini API returns an error or a malformed response, the integration layer will catch it and send a proper `event: error` message to the frontend.
- **Network Issues**: The frontend will have built-in retry logic for dropped connections during streaming.
- **Content Moderation**: All user inputs and AI outputs will be passed through content moderation filters. 